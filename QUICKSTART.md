# ğŸš€ QUICKSTART - Lancer l'entraÃ®nement MAINTENANT

## âœ… Tout est PRÃŠT !

Toutes les corrections sont faites. Lance simplement :

```bash
python train_optimized.py
```

---

## ğŸ“‹ Checklist des corrections appliquÃ©es

### âœ… 1. reward_parts propagÃ©s
- **Fichier** : `src/utils/env_wrapper.py` (RewardPartsWrapper)
- **Status** : âœ… CorrigÃ©
- **Impact** : TensorBoard affichera maintenant les graphes `reward/goal`, `reward/touch`, etc.

### âœ… 2. Callbacks branchÃ©s
- **Fichiers** :
  - `src/utils/custom_callbacks.py` (CustomTBCallback + EvalCallback)
  - `train_optimized.py` (ligne 30-31)
- **Status** : âœ… BranchÃ©s dans train_optimized.py
- **Impact** :
  - Logging enrichi (reward breakdown, gradients, success rate)
  - Ã‰valuation auto toutes les 50k steps
  - Sauvegarde best model automatique

### âœ… 3. tick_skip corrigÃ©
- **Fichier** : `config.yaml` (ligne 24)
- **Valeur** : `tick_skip: 8` (Ã©tait 160 âŒ)
- **Status** : âœ… CorrigÃ©
- **Impact** : Signal d'apprentissage 20x meilleur !

### âœ… 4. Throughput PPO alignÃ©
- **Fichier** : `config.yaml` (ligne 6-7)
- **Formule** : `num_envs (8) Ã— n_steps (512) = batch_size (4096)`
- **Status** : âœ… AlignÃ©
- **Impact** : Batches propres, pas de padding, meilleure efficacitÃ© GPU

### âœ… 5. CUDA optimisÃ©
- **Fichier** : `train_optimized.py` (ligne 34-49)
- **Optimisations** :
  - âœ… `torch.set_float32_matmul_precision('high')` (TF32 sur Ampere+)
  - âœ… `torch.backends.cudnn.benchmark = True`
  - âœ… `torch.compile(policy)` (PyTorch 2.0+)
- **Status** : âœ… ActivÃ©
- **Impact** : -5% Ã  -20% de temps d'entraÃ®nement

---

## ğŸ¯ Commandes rapides

### 1. Lancer l'entraÃ®nement

```bash
python train_optimized.py
```

### 2. Monitorer en temps rÃ©el

**Terminal 2** :
```bash
tensorboard --logdir=./logs
```

Puis ouvre **http://localhost:6006**

### 3. VÃ©rifier CUDA (optionnel)

```bash
python -c "import torch; print('CUDA:', torch.cuda.is_available())"
python -c "import torch; print('GPU:', torch.cuda.get_device_name(0))"
```

---

## ğŸ“Š Ce que tu vas voir dans TensorBoard

### Graphes disponibles

| CatÃ©gorie | MÃ©triques | Description |
|-----------|-----------|-------------|
| **rollout/** | `ep_rew_mean`, `ep_len_mean` | RÃ©compense et longueur moyennes des Ã©pisodes |
| **reward/** | `goal_mean`, `touch_mean`, `progress_mean`, `boost_mean`, `demo_mean`, `aerial_mean` | Breakdown des composantes de rÃ©compense |
| **reward/** | `goal_hist`, `touch_hist`, etc. | Histogrammes des distributions |
| **train/** | `policy_loss`, `value_loss`, `entropy_loss` | Pertes d'entraÃ®nement |
| **train/** | `grad_norm` | Norme des gradients (dÃ©tecte vanishing/exploding) |
| **metrics/** | `success_rate` | % de buts marquÃ©s |
| **eval/** | `mean_reward`, `std_reward` | Performance en Ã©valuation dÃ©terministe |

### Ã€ surveiller

ğŸŸ¢ **Bon signe** :
- `rollout/ep_rew_mean` augmente
- `reward/goal_mean` > 0 (buts marquÃ©s !)
- `metrics/success_rate` > 0.1 (aprÃ¨s ~100k steps)
- `train/grad_norm` entre 0.1 et 10

ğŸ”´ **Mauvais signe** :
- `train/grad_norm` > 100 â†’ gradients explosent (rÃ©duis `learning_rate`)
- `train/grad_norm` < 0.001 â†’ gradients s'annulent (augmente `learning_rate`)
- `rollout/ep_rew_mean` stagne â†’ vÃ©rifie les reward weights

---

## âš™ï¸ Configuration actuelle

```yaml
# EntraÃ®nement
total_timesteps: 10M
batch_size: 4096 (8 envs Ã— 512 steps)
learning_rate: 0.0003
n_epochs: 4
seed: 42

# Environnement
num_envs: 8 (auto-ajustÃ© selon CPU)
tick_skip: 8 (15 FPS)
team_size: 1v1

# RÃ©seau
policy_layers: [256, 256, 256]
value_layers: [256, 256, 256]

# RÃ©compenses clÃ©s
goal: +100
concede: -100
velocity_ball_to_goal: +2.0
touch: +1.0
```

---

## ğŸ› DÃ©pannage rapide

### Erreur : "CUDA out of memory"

```yaml
# config.yaml
training:
  batch_size: 2048  # Au lieu de 4096
  n_steps: 256      # Au lieu de 512

environment:
  num_envs: 4  # Au lieu de 8
```

### Erreur : "rlviser not found"

**C'est normal !** RLViser est optionnel. L'entraÃ®nement fonctionne sans.

### Performances lentes (< 1000 steps/sec)

1. VÃ©rifie CUDA : `nvidia-smi`
2. Utilise `train_optimized.py` (pas `train.py`)
3. RÃ©duis `num_envs` si CPU limitÃ©

### Reward ne monte pas aprÃ¨s 100k steps

1. VÃ©rifie `config.yaml` â†’ `tick_skip: 8` (pas 160 !)
2. Regarde `reward/*/hist` dans TensorBoard
3. Augmente `learning_rate: 0.001` temporairement

---

## ğŸ“ˆ Timeline attendue

| Steps | Comportement attendu |
|-------|---------------------|
| 0-10k | Random, reward ~0 |
| 10k-50k | Commence Ã  toucher la balle |
| 50k-200k | Se dirige vers la balle, premiers buts |
| 200k-500k | Buts rÃ©guliers, meilleure dÃ©fense |
| 500k-1M | StratÃ©gies plus avancÃ©es |
| 1M+ | AmÃ©lioration continue |

---

## ğŸ® Fichiers gÃ©nÃ©rÃ©s

```
Botrl/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model.pth       # â­ Meilleur modÃ¨le (selon Ã©val)
â”‚   â”œâ”€â”€ checkpoint_100k.pth  # Sauvegardes toutes les 100k
â”‚   â”œâ”€â”€ checkpoint_200k.pth
â”‚   â””â”€â”€ model_final.pth      # ModÃ¨le final
â”‚
â””â”€â”€ logs/
    â””â”€â”€ events.out.tfevents.* # Logs TensorBoard
```

---

## ğŸš€ GO !

**Tout est prÃªt. Lance maintenant :**

```bash
python train_optimized.py
```

**Et dans un autre terminal :**

```bash
tensorboard --logdir=./logs
```

**Bon entraÃ®nement ! âš½ğŸ¤–**

---

## ğŸ’¡ Tips avancÃ©s

### AccÃ©lÃ©rer encore plus

1. **Augmente num_envs** (si CPU le permet) :
   ```yaml
   environment:
     num_envs: 12
   training:
     n_steps: 341  # Pour garder 12 Ã— 341 â‰ˆ 4096
   ```

2. **Active Weights & Biases** :
   ```yaml
   logging:
     wandb: true
   ```

3. **Teste diffÃ©rents tick_skip** :
   - `tick_skip: 8` â†’ 15 FPS (recommandÃ©)
   - `tick_skip: 12` â†’ 10 FPS (plus rapide Ã  entraÃ®ner)
   - `tick_skip: 16` â†’ 7.5 FPS (compromis)

### Tweaker les rewards

Si le bot ne marque pas assez de buts aprÃ¨s 500k steps :

```yaml
rewards:
  goal_weight: 200.0  # â¬†ï¸ Double reward
  velocity_ball_to_goal_weight: 5.0  # â¬†ï¸ Encourage plus
```

Si le bot reste collÃ© Ã  la balle sans tirer :

```yaml
rewards:
  velocity_ball_to_goal_weight: 10.0  # â¬†ï¸â¬†ï¸ Force Ã  pousser vers le but
  align_ball_goal_weight: 3.0  # â¬†ï¸ Bonus alignement
```

---

**Questions ? Lance et observe les courbes TensorBoard ! ğŸ“Š**
